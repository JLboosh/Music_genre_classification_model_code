{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ccda84-8051-4351-ad45-3d0c3b37f6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacky\\AppData\\Local\\Temp\\ipykernel_10600\\2924223113.py:46: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(\n",
      "C:\\Users\\Jacky\\anaconda3\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file C:\\Users\\Jacky\\Documents\\fma_small\\fma_small\\098\\098565.mp3 : \n",
      "Skipping file C:\\Users\\Jacky\\Documents\\fma_small\\fma_small\\098\\098567.mp3 : \n",
      "Skipping file C:\\Users\\Jacky\\Documents\\fma_small\\fma_small\\098\\098569.mp3 : \n",
      "Skipping file C:\\Users\\Jacky\\Documents\\fma_small\\fma_small\\099\\099134.mp3 : \n",
      "Skipping file C:\\Users\\Jacky\\Documents\\fma_small\\fma_small\\108\\108925.mp3 : \n",
      "Skipping file C:\\Users\\Jacky\\Documents\\fma_small\\fma_small\\133\\133297.mp3 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacky\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - accuracy: 0.1325 - loss: 5.1062 - val_accuracy: 0.1400 - val_loss: 2.0785\n",
      "Epoch 2/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - accuracy: 0.2419 - loss: 1.9902 - val_accuracy: 0.2700 - val_loss: 1.9591\n",
      "Epoch 3/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - accuracy: 0.3938 - loss: 1.6963 - val_accuracy: 0.3150 - val_loss: 1.8092\n",
      "Epoch 4/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - accuracy: 0.6175 - loss: 1.1675 - val_accuracy: 0.3100 - val_loss: 1.9305\n",
      "Epoch 5/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - accuracy: 0.8450 - loss: 0.5647 - val_accuracy: 0.3400 - val_loss: 2.2353\n",
      "Epoch 6/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 2s/step - accuracy: 0.9606 - loss: 0.1983 - val_accuracy: 0.3400 - val_loss: 2.3782\n",
      "Epoch 7/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - accuracy: 0.9925 - loss: 0.0633 - val_accuracy: 0.3400 - val_loss: 2.7589\n",
      "Epoch 8/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.9994 - loss: 0.0225 - val_accuracy: 0.3650 - val_loss: 2.9475\n",
      "Epoch 9/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0095 - val_accuracy: 0.3700 - val_loss: 3.1300\n",
      "Epoch 10/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0055 - val_accuracy: 0.3550 - val_loss: 3.3317\n",
      "Epoch 11/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.3450 - val_loss: 3.4497\n",
      "Epoch 12/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.3600 - val_loss: 3.6005\n",
      "Epoch 13/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.3650 - val_loss: 3.6266\n",
      "Epoch 14/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.3600 - val_loss: 3.7158\n",
      "Epoch 15/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 7.3341e-04 - val_accuracy: 0.3650 - val_loss: 3.7411\n",
      "Epoch 16/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 5.6354e-04 - val_accuracy: 0.3650 - val_loss: 3.7658\n",
      "Epoch 17/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 4.6693e-04 - val_accuracy: 0.3700 - val_loss: 3.7939\n",
      "Epoch 18/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 3.9178e-04 - val_accuracy: 0.3700 - val_loss: 3.8393\n",
      "Epoch 19/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 3.3606e-04 - val_accuracy: 0.3700 - val_loss: 3.8382\n",
      "Epoch 20/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.9194e-04 - val_accuracy: 0.3750 - val_loss: 3.8606\n",
      "Epoch 1/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.1562 - loss: 4.2810 - val_accuracy: 0.1850 - val_loss: 2.0555\n",
      "Epoch 2/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - accuracy: 0.1838 - loss: 2.0405 - val_accuracy: 0.2400 - val_loss: 1.9648\n",
      "Epoch 3/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - accuracy: 0.2025 - loss: 1.9755 - val_accuracy: 0.1700 - val_loss: 1.9325\n",
      "Epoch 4/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - accuracy: 0.2362 - loss: 1.9350 - val_accuracy: 0.3500 - val_loss: 1.8776\n",
      "Epoch 5/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.2600 - loss: 1.8643 - val_accuracy: 0.3500 - val_loss: 1.7663\n",
      "Epoch 6/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 2s/step - accuracy: 0.2700 - loss: 1.7839 - val_accuracy: 0.3500 - val_loss: 1.7724\n",
      "Epoch 7/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - accuracy: 0.2844 - loss: 1.7628 - val_accuracy: 0.3750 - val_loss: 1.7610\n",
      "Epoch 8/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - accuracy: 0.3244 - loss: 1.6422 - val_accuracy: 0.3650 - val_loss: 1.7514\n",
      "Epoch 9/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.3288 - loss: 1.5929 - val_accuracy: 0.3500 - val_loss: 1.6886\n",
      "Epoch 10/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 2s/step - accuracy: 0.3713 - loss: 1.5151 - val_accuracy: 0.3300 - val_loss: 1.6845\n",
      "Epoch 11/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - accuracy: 0.3625 - loss: 1.5203 - val_accuracy: 0.3750 - val_loss: 1.6457\n",
      "Epoch 12/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.3850 - loss: 1.4308 - val_accuracy: 0.3950 - val_loss: 1.7013\n",
      "Epoch 13/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - accuracy: 0.3881 - loss: 1.4062 - val_accuracy: 0.4050 - val_loss: 1.6617\n",
      "Epoch 14/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - accuracy: 0.4169 - loss: 1.3190 - val_accuracy: 0.3600 - val_loss: 1.7220\n",
      "Epoch 15/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - accuracy: 0.4419 - loss: 1.2942 - val_accuracy: 0.3650 - val_loss: 1.7245\n",
      "Epoch 16/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - accuracy: 0.4325 - loss: 1.3056 - val_accuracy: 0.3100 - val_loss: 1.7471\n",
      "Epoch 17/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 2s/step - accuracy: 0.4487 - loss: 1.3092 - val_accuracy: 0.3700 - val_loss: 1.7549\n",
      "Epoch 18/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - accuracy: 0.4594 - loss: 1.2271 - val_accuracy: 0.3850 - val_loss: 1.7536\n",
      "Epoch 19/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.4681 - loss: 1.2066 - val_accuracy: 0.3850 - val_loss: 1.9778\n",
      "Epoch 20/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.4606 - loss: 1.2184 - val_accuracy: 0.3650 - val_loss: 1.8452\n",
      "Epoch 1/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.1358 - loss: 108.0372  "
     ]
    }
   ],
   "source": [
    "import resampy\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    ")\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "# load FMA dataset - they are weird they put the genres in a .csv file\n",
    "TRACKS_CSV = r\"C:\\Users\\Jacky\\Documents\\fma_metadata\\tracks.csv\"\n",
    "tracks = pd.read_csv(TRACKS_CSV, index_col = 0, header = [0, 1])\n",
    "\n",
    "# get genre labels for FMA-small\n",
    "genres = tracks['track']['genre_top']\n",
    "\n",
    "# dataset path\n",
    "DATASET_PATH = r\"C:\\Users\\Jacky\\Documents\\fma_small\\fma_small\"\n",
    "\n",
    "# converting audio to mel-spectrograms\n",
    "# parameters\n",
    "SR = 22050 # sampling rate --> default to librosa\n",
    "N_FFT = 2048 # window size\n",
    "HOP_LENGTH = 512 # 1/4th of window size\n",
    "N_MELS = 128 \n",
    "DURATION = 30 # GTZAN clips duration \n",
    "FIXED_FRAMES = 1290\n",
    "\n",
    "\n",
    "# feature extraction function\n",
    "def extract_mel_spectrogram(file_path):\n",
    "    try:\n",
    "        audio, sr = librosa.load(\n",
    "            file_path,\n",
    "            sr = SR,\n",
    "            duration = DURATION,\n",
    "            res_type = \"kaiser_fast\"\n",
    "        )\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y = audio,\n",
    "            sr = sr,\n",
    "            n_fft = N_FFT,\n",
    "            hop_length = HOP_LENGTH,\n",
    "            n_mels = N_MELS\n",
    "        )\n",
    "\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref = np.max)\n",
    "       \n",
    "        # truncate to fixed length\n",
    "        if mel_spec_db.shape[1] < FIXED_FRAMES:\n",
    "            pad_width = FIXED_FRAMES - mel_spec_db.shape[1]\n",
    "            mel_spec_db = np.pad(\n",
    "                mel_spec_db,\n",
    "                pad_width = ((0, 0), (0, pad_width)),\n",
    "                mode = 'constant'\n",
    "            )\n",
    "        else:\n",
    "            mel_spec_db = mel_spec_db[:, :FIXED_FRAMES]\n",
    "            \n",
    "        return mel_spec_db\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {file_path} : {e}\")\n",
    "        return None\n",
    "\n",
    "# FMA dataset builder\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for root, _, files in os.walk(DATASET_PATH):\n",
    "    for file in files:\n",
    "        if file.endswith(\".mp3\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            track_id = int(file.replace(\".mp3\", \"\"))\n",
    "\n",
    "            genre = genres.get(track_id)\n",
    "            if pd.isna(genre):\n",
    "                continue\n",
    "\n",
    "            mel_spec = extract_mel_spectrogram(file_path)\n",
    "            if mel_spec is not None:\n",
    "                X.append(mel_spec)\n",
    "                y.append(genre)\n",
    "\n",
    "# limiting number of samples based on my RAM\n",
    "X, y = shuffle(X, y, random_state = 42)\n",
    "\n",
    "MAX_SAMPLES = 2000 \n",
    "\n",
    "X = X[:MAX_SAMPLES]\n",
    "y = y[:MAX_SAMPLES]\n",
    "\n",
    "# convert X to numpy\n",
    "X = np.array(X, dtype = np.float32)\n",
    "\n",
    "# encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# 80/10/10 split\n",
    "X_train, X_temp, y_train_enc, y_temp_enc = train_test_split(\n",
    "    X, y_encoded, test_size = 0.2, random_state = 42, stratify = y_encoded\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val_enc, y_test_enc = train_test_split(\n",
    "    X_temp, y_temp_enc, test_size = 0.5, random_state = 42, stratify = y_temp_enc\n",
    ")\n",
    "\n",
    "y_train = to_categorical(y_train_enc)\n",
    "y_val = to_categorical(y_val_enc)\n",
    "y_test = to_categorical(y_test_enc)\n",
    "\n",
    "# compute mean and standard deviation from training set\n",
    "mu = np.mean(X_train)\n",
    "sigma = np.std(X_train)\n",
    "\n",
    "X_train = (X_train - mu) / sigma\n",
    "X_val = (X_val - mu) / sigma\n",
    "X_test = (X_test - mu) / sigma\n",
    "\n",
    "# reshape for CNN input\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_val = X_val[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "# set constant learning rate\n",
    "LR = 0.001\n",
    "\n",
    "# CNN architecture (base model)\n",
    "def build_base_cnn():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation = 'relu', input_shape = input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        Conv2D(64, (3,3), activation = 'relu'), \n",
    "        MaxPooling2D((2, 2)),   \n",
    "\n",
    "        Flatten(),\n",
    "        Dense(64, activation = 'relu'),\n",
    "        Dense(num_classes, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "# model variants\n",
    "\n",
    "# 1 no regularization (basline for comparison)\n",
    "baseline_model = build_base_cnn()\n",
    "\n",
    "baseline_model.compile(\n",
    "    optimizer = Adam(learning_rate = LR),\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# Dropout model\n",
    "def build_dropout_cnn():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation = 'relu', input_shape = input_shape),\n",
    "        MaxPooling2D((2,2)),\n",
    "\n",
    "        Conv2D(64, (3,3), activation = 'relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(64, activation = 'relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation = 'softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "dropout_model = build_dropout_cnn()\n",
    "dropout_model.compile(\n",
    "    optimizer = Adam(learning_rate = LR),\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# batch normalization model\n",
    "def build_batchnorm_cnn():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), use_bias = False, input_shape = input_shape),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "\n",
    "        Conv2D(64, (3,3), use_bias = False),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(64, activation = 'relu'),\n",
    "        Dense(num_classes, activation = 'softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "batchnorm_model = build_batchnorm_cnn()\n",
    "batchnorm_model.compile(\n",
    "    optimizer = Adam(learning_rate = LR),\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# training (no early stopping) \n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "history_baseline = baseline_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data = (X_val, y_val),\n",
    "    epochs = EPOCHS,\n",
    "    batch_size = BATCH_SIZE\n",
    ")\n",
    "\n",
    "history_dropout = dropout_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data = (X_val, y_val),\n",
    "    epochs = EPOCHS,\n",
    "    batch_size = BATCH_SIZE\n",
    ")\n",
    "\n",
    "history_batchnorm = batchnorm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data = (X_val, y_val),\n",
    "    epochs = EPOCHS,\n",
    "    batch_size = BATCH_SIZE\n",
    ")\n",
    "\n",
    "# evaluation\n",
    "baseline_test_acc = baseline_model.evaluate(X_test, y_test, verbose = 0)[1]\n",
    "dropout_test_acc = dropout_model.evaluate(X_test, y_test, verbose = 0)[1]\n",
    "batchnorm_test_acc = batchnorm_model.evaluate(X_test, y_test, verbose = 0)[1]\n",
    "\n",
    "print(\"Baseline Test Accuracy:\", baseline_test_acc)\n",
    "print(\"Dropout Test Accuracy:\", dropout_test_acc)\n",
    "print(\"BatchNorm Test Accuracy:\", batchnorm_test_acc)\n",
    "\n",
    "# analyzing overfitting issue\n",
    "\n",
    "def plot_history(history, title):\n",
    "    plt.plot(history.history['accuracy'], label = 'Train')\n",
    "    plt.plot(history.history['val_accuracy'], label = 'Validation')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history_baseline, \"Baseline Model\")\n",
    "plot_history(history_dropout, \"Dropout Model\")\n",
    "plot_history(history_batchnorm, \"Batch Normalization Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa34a43-c16d-492b-8c16-4fdacc53c1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f264376-f754-4a42-a3d1-dbdd327418ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
